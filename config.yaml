## model hyperparameters
context_len: 1024  # called block_size in the video
n_embed: 768
vocab_size: 50304
n_layer: 12
n_head: 12
init_linear_std: 0.02

## Learning hyperparameters
batch_size: 524288  # in number of tokens. so actual "batch size" would be this number divided by the context length
minibatch_size: 64   # max this to just fit into your GPU
max_steps: 38146  # one epoch is roughly 19073 steps
eval_interval: 250
val_steps: 20
adam_betas: [0.9, 0.95]
adam_eps: 1.0e-8
lr_max: 18.0e-4
lr_min: 6.0e-5
lr_warmup_steps: 715
lr_max_steps: 50
weight_decay: 0.1
dataloader_nworkers: 4

## Other Config
use_wandb: True
use_compile: True