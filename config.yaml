## model hyperparameters
context_len: 1024  # called block_size in the video
n_embed: 768
vocab_size: 50304
n_layer: 12
n_head: 12
init_linear_std: 0.02

## Learning hyperparameters
batch_size: 65536 # 524288  # in number of tokens. so actual "batch size" would be this number divided by the context length
minibatch_size: 8   # max this to just fit into your GPU
epochs: 10
eval_interval: 500
eval_iters: 200
learning_rate: 3.0e-4
dropout: 0.2
adam_betas: [0.9, 0.95]
adam_eps: 1.0e-8
lr_max: 6.0e-4
lr_min: 6.0e-5
lr_warmup_steps: 10
lr_max_steps: 50
weight_decay: 0.1

## Other Config
use_wandb: False